{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа 10\n",
    "\n",
    "ФИО: Филимонов Степан"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQkp36rEoScR"
   },
   "source": [
    "Задание: обучите модель классификации букв для задачи расстановки ударения с помощью методов из библиотеки transformers. Датасет для обучения можно взять отсюда: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n",
    "\n",
    "1. Напишите класс для Dataset/Dataloder и разбейте данные на случайные train / test сплиты в соотношении 50:50. (1 балл)\n",
    "2. Попробуйте обучить одну или несколько из моделей: Bert, Albert, Deberta. Посчитайте метрику Accuracy на train и test. (1 балл). При преодолении порога в Accuracy на test 0.8: (+1 балл), 0.85: (+2 балла), 0.89: (+3 балла).\n",
    "Пример конфигурации для deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import datetime\n",
    "import progressbar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch import device, cuda\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEVICE = device(\"cuda:0\" if cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>аболиционисский</td>\n",
       "      <td>аболицион^исский</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849248</th>\n",
       "      <td>однопользовательских</td>\n",
       "      <td>одноп^ользовательских</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301354</th>\n",
       "      <td>растратишь</td>\n",
       "      <td>растр^атишь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050413</th>\n",
       "      <td>подновляемый</td>\n",
       "      <td>подновл^яемый</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53542</th>\n",
       "      <td>ассимилированными</td>\n",
       "      <td>ассимил^ированными</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title                 accent\n",
       "1551          аболиционисский       аболицион^исский\n",
       "849248   однопользовательских  одноп^ользовательских\n",
       "1301354            растратишь            растр^атишь\n",
       "1050413          подновляемый          подновл^яемый\n",
       "53542       ассимилированными     ассимил^ированными"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_accents.tsv', delimiter='\\t', names=['title', 'accent'])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас будет задача классификации, где буква на которую падает ударение, есть истенный класс. Для этого в блоке `accent`\n",
    "оставим только номер гласной, на которую падает уравнение, начиная с *1*. Если значение равно 0 - в слове нет ударения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Колличество данных: 1,680,535\n",
      "Кол-во классов: 56\n"
     ]
    }
   ],
   "source": [
    "df['accent'] = [ ( word.find('^') + 1 ) for word in  df['accent']]\n",
    "NUM_CLASSES = (max(df[\"accent\"]) - min(df[\"accent\"]) + 1)\n",
    "df['accent'] = [[ 0.0 if i != acc else 1.0 for i in range(NUM_CLASSES)] for acc in df['accent']]\n",
    "print(f'Колличество данных: {df.shape[0]:,}')\n",
    "print(f'Кол-во классов: {NUM_CLASSES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1680535/1680535 [01:31<00:00, 18277.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальный размер предложения:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = max([len(tokenizer.encode(sent, add_special_tokens=True)) for sent in tqdm(df['title'])])\n",
    "print('Максимальный размер предложения: ', MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1680535/1680535 [02:20<00:00, 11929.80it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tqdm(df['title']):\n",
    "    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True, max_length=MAX_LENGTH,\n",
    "                                         pad_to_max_length=True, return_attention_mask=True,\n",
    "                                         return_tensors='pt', truncation=True)\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(df['accent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 840,267\n",
      "Валидация: 840,268\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataset, val_dataset = train_test_split(dataset, train_size=0.5, random_state=42)\n",
    "print(f'Обучающая выборка: {len(train_dataset):,}')\n",
    "print(f'Валидация: {len(val_dataset):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=56, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=NUM_CLASSES,\n",
    "                                                      output_attentions=False, output_hidden_states=False,).to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    _, lbls = torch.max(labels, 1)\n",
    "    return torch.sum(preds == lbls).item() / lbls.shape[0]\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 4 ======== \tTotal time: 0:00:00 s. \n",
      "\tTraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [34:59<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 0.07.  Training accuracy: 0.14. Time: 0:35:00 s. \n",
      " \n",
      "\tRunning Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [06:28<00:00, 67.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal loss: 0.07.  Val accuracy: 0.15. Time: 0:06:29 s.\n",
      "======== Epoch 2 / 4 ======== \tTotal time: 0:41:28 s. \n",
      "\tTraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [35:00<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 0.07.  Training accuracy: 0.14. Time: 0:35:00 s. \n",
      " \n",
      "\tRunning Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [06:29<00:00, 67.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal loss: 0.06.  Val accuracy: 0.08. Time: 0:06:30 s.\n",
      "======== Epoch 3 / 4 ======== \tTotal time: 1:22:58 s. \n",
      "\tTraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [34:58<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 0.07.  Training accuracy: 0.14. Time: 0:34:59 s. \n",
      " \n",
      "\tRunning Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [06:30<00:00, 67.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal loss: 0.08.  Val accuracy: 0.16. Time: 0:06:30 s.\n",
      "======== Epoch 4 / 4 ======== \tTotal time: 2:04:27 s. \n",
      "\tTraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [34:57<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss: 0.07.  Training accuracy: 0.14. Time: 0:34:58 s. \n",
      " \n",
      "\tRunning Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 26259/26259 [06:30<00:00, 67.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal loss: 0.08.  Val accuracy: 0.03. Time: 0:06:30 s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_epochs, train_accuracy_epochs = [], []\n",
    "val_loss_epochs, val_accuracy_epochs = [], []\n",
    "\n",
    "t = time.time()\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(\n",
    "        f'======== Epoch {(epoch_i + 1)} / {EPOCHS} ========',\n",
    "        f'\\tTotal time: {format_time(time.time() - t)} s.',\n",
    "        f'\\n\\tTraining...'\n",
    "    )\n",
    "    t0 = time.time()    \n",
    "    \n",
    "    total_train_loss, total_train_accuracy, total_eval_loss, total_eval_accuracy = 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):        \n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).logits\n",
    "        loss = loss_foo(output, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        total_train_accuracy += accuracy(output, b_labels)\n",
    "    \n",
    "    train_loss_epochs.append(total_train_loss / len(train_dataloader))\n",
    "    train_accuracy_epochs.append(total_train_accuracy / len(train_dataloader))\n",
    "\n",
    "    print(\n",
    "        f'\\tTraining loss: {train_loss_epochs[-1]:.2f}.',\n",
    "        f' Training accuracy: {train_accuracy_epochs[-1]:.2f}.'\n",
    "        f' Time: {format_time(time.time() - t0)} s.',\n",
    "        f'\\n',\n",
    "        f'\\n\\tRunning Validation...'\n",
    "    )\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).logits\n",
    "        \n",
    "        loss = loss_foo(output, b_labels)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        total_eval_accuracy += accuracy(output, b_labels)\n",
    "\n",
    "    val_accuracy_epochs.append(total_eval_accuracy / len(validation_dataloader))\n",
    "    val_loss_epochs.append(total_eval_loss / len(validation_dataloader))\n",
    "    \n",
    "    print(\n",
    "        f'\\tVal loss: {val_loss_epochs[-1]:.2f}.',\n",
    "        f' Val accuracy: {val_accuracy_epochs[-1]:.2f}.',\n",
    "        f'Time: {format_time(time.time() - t0)} s.'\n",
    "    )\n",
    "    \n",
    "    if val_accuracy_epochs[-1] >= 0.9:\n",
    "        print('На обучающей и тестовой выборке достигли желаемого результата.\\n',\n",
    "              'Чтобы не израходовать ресурсы машины:\\t break')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** \n",
    "\n",
    "#### До того как у меня появился доступ к gpu\n",
    "~~Вычисления все полностью выходили бы примерно ~21 час. Исходя из опыта предыдущих домашних работ, если использовать весь массив,\n",
    "для обучения и валидации модели, то для получения минимального loss достаточно пройти обучение, для экономии ресурса машины я решил до конца недообучать.~~\n",
    "\n",
    "~~P.s. до конца семестра хочу получить доступ к машине с GPU, произведу пересчет данной задачи и дополню решение на портале~~\n",
    "\n",
    "#### После того как у меня появился доступ к gpu\n",
    "\n",
    "1. Не получилось достичь, accuracy 0.9. Похоже идея что номер ударения равен классу оказалась не удачной.\n",
    "\n",
    "2. Похоже много классовой классификацией не стоит увлекатся, как-то очень размазано все вышло.\n",
    "\n",
    "3. Без gpu я бы этого не понял, мне казалось изначально, что низкая accuracy получается из за того, что просто малую выборку выбираю"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
